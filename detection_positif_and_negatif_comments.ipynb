{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNWGjKwsBYTc9uQrNYml1em",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekaadev/detection_comments/blob/main/detection_positif_and_negatif_comments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Library & Dependency"
      ],
      "metadata": {
        "id": "PsJ570thDmWZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZRHhsTThE-K",
        "outputId": "03f3fe3a-f2bf-4e25-8ffc-1ded9de0a34d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ],
      "source": [
        "# import dependency\n",
        "\n",
        "# install gensim - pengolahan bahasa alami, scikit-learn(lib) - machine learning library\n",
        "! pip install gensim scikit-learn -U\n",
        "# add library\n",
        "\n",
        "# gensim, library yang digunakan untuk pemrosesan baahsa alami(nlp)\n",
        "import gensim\n",
        "# logging, memberikan log / output pada saat program dijalankan\n",
        "import logging\n",
        "# import tagged document, untuk klasifikasikan dari sebuah kata yang telah diberi label\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "# import model doc 2 vec\n",
        "from gensim.models import Doc2Vec\n",
        "\n",
        "# import random\n",
        "import random\n",
        "# import regex (regular expression)\n",
        "import re\n",
        "# import os untuk konfigurasi operating system\n",
        "import os\n",
        "# import numpy untuk pengolahan angka\n",
        "import numpy as np\n",
        "\n",
        "# import KNeighborsClassifier, untuk implementasi algoritma KNN\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "# import RandomForestClassifier, untuk implementasi algoritma RandomForest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# import cross val predict, untuk generate estimasi nilai cross validated tiap input dari sebuah data\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "# cosine similarity menghitung kemiripan dari 2 vektor\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "# constructor pipeline dari input estimasi yang diberikan\n",
        "from sklearn.pipeline import make_pipeline\n",
        "# import CountVectorizer, untuk mengkovert koleksi dari sebuah text document ke dalam matrix, TfidfTransformer, menstransformasikan total matrix menjadi tf atau tfid yang normal (tfidf, cara menentukan kata itu penting atau tidak dalam suatu dokumen terhadap kumpulan dokumen menggunakan metode statistik)\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download dataset dan model pretrained"
      ],
      "metadata": {
        "id": "8AkcR8AGDr2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# import data\n",
        "# googlenewvectornegative300\n",
        "# stanford sentiment treebank\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"leadbest/googlenewsvectorsnegative300\")\n",
        "path2 = kagglehub.dataset_download(\"atulanandjha/stanford-sentiment-treebank-v2-sst2\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "print(\"Path to dataset files:\", path2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7IzIpCVOVrP",
        "outputId": "92363788-d4c5-492d-9a30-9916fa413d99"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/atulanandjha/stanford-sentiment-treebank-v2-sst2?dataset_version_number=30...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19.1M/19.1M [00:00<00:00, 143MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/googlenewsvectorsnegative300\n",
            "Path to dataset files: /root/.cache/kagglehub/datasets/atulanandjha/stanford-sentiment-treebank-v2-sst2/versions/30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Memuat model pretained"
      ],
      "metadata": {
        "id": "MPm_jkzgDxDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# memuat models\n",
        "gmodel = gensim.models.KeyedVectors.load_word2vec_format('/kaggle/input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin', binary=True)"
      ],
      "metadata": {
        "id": "Rm0IziRHLMAv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Membuat fungsi word extract\n",
        "\n",
        "Fungsi ini digunakan untuk membersihkan sebuah kata agar kata yang dikumpulkan sudah sesuai dengan yang dibutuhkan"
      ],
      "metadata": {
        "id": "mfzpf1j1D48h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fungsi regex\n",
        "def word_extract(word):\n",
        "  # lowercase\n",
        "  word = word.lower()\n",
        "  # menghilangkan tag html\n",
        "  word = re.sub(r'<[^>]+>', ' ', word)\n",
        "  # menghilangkan tanda petik diantara sebuah kata misal don't -> dont\n",
        "  word = re.sub(r'(\\w)\\'(\\w)', '\\1\\2', word)\n",
        "  # mengganti selain alpha numerik menjadi spasi\n",
        "  word = re.sub(r'\\W', ' ', word)\n",
        "  # mengganti spasi berlebihan menjadi satu spasi saja\n",
        "  word = re.sub(r'\\s+', ' ', word)\n",
        "  # menghilangkan spasi diawal dan diakhir kata\n",
        "  word = word.strip()\n",
        "  # memisahkan kata menjadi per spasi\n",
        "  return word.split()"
      ],
      "metadata": {
        "id": "-bHvBT_YPFIv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Menambahkan data"
      ],
      "metadata": {
        "id": "SkfIPGtTEG0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# unsupervised training data\n",
        "unsup_sentences = []\n",
        "\n",
        "# menambahkan data yang sudah diberikan tag ke dalam unsup_sentences, data tersebut berisi kata dari setiap file (imdb review, moview_review, stanford sentiment treebank)\n",
        "\n",
        "!wget -P /tmp https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xzf /tmp/aclImdb_v1.tar.gz -C /tmp\n",
        "for dirname in ['/tmp/aclImdb/train/neg', '/tmp/aclImdb/train/pos', '/tmp/aclImdb/test/neg', '/tmp/aclImdb/test/pos']:\n",
        "  for filename in os.listdir(dirname):\n",
        "    with open(os.path.join(dirname, filename), 'r') as f:\n",
        "      words=word_extract(f.read())\n",
        "      unsup_sentences.append(TaggedDocument(words, [dirname + '/' + filename]))\n",
        "\n",
        "!wget -P /tmp http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz\n",
        "!tar -xzf /tmp/review_polarity.tar.gz -C /tmp\n",
        "for dirname in ['/tmp/txt_sentoken/pos', '/tmp/txt_sentoken/neg']:\n",
        "  for filename in os.listdir(dirname):\n",
        "    with open(os.path.join(dirname, filename), 'r') as f:\n",
        "      for i, sended in enumerate(f):\n",
        "        words = word_extract(sended)\n",
        "        unsup_sentences.append(TaggedDocument(words, ['%s/%s-%d' % (dirname, filename, i)]))\n",
        "\n",
        "with open('/root/.cache/kagglehub/datasets/atulanandjha/stanford-sentiment-treebank-v2-sst2/versions/30/SST2-Data/SST2-Data/stanfordSentimentTreebank/stanfordSentimentTreebank/original_rt_snippets.txt', 'r', encoding='utf-8') as f:\n",
        "    for i, sended in enumerate(f):\n",
        "        words = word_extract(sended)\n",
        "        unsup_sentences.append(TaggedDocument(words, [f'rt-{i}']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jR9sTeIEQkYa",
        "outputId": "70311fc6-d904-4ac7-f1ed-d3b31c5aa404"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-10 16:24:00--  https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘/tmp/aclImdb_v1.tar.gz’\n",
            "\n",
            "aclImdb_v1.tar.gz   100%[===================>]  80.23M  67.3MB/s    in 1.2s    \n",
            "\n",
            "2025-05-10 16:24:01 (67.3 MB/s) - ‘/tmp/aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n",
            "--2025-05-10 16:24:21--  http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz\n",
            "Resolving www.cs.cornell.edu (www.cs.cornell.edu)... 132.236.207.53\n",
            "Connecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.53|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz [following]\n",
            "--2025-05-10 16:24:21--  https://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz\n",
            "Connecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.53|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3127238 (3.0M) [application/x-gzip]\n",
            "Saving to: ‘/tmp/review_polarity.tar.gz’\n",
            "\n",
            "review_polarity.tar 100%[===================>]   2.98M  5.19MB/s    in 0.6s    \n",
            "\n",
            "2025-05-10 16:24:22 (5.19 MB/s) - ‘/tmp/review_polarity.tar.gz’ saved [3127238/3127238]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(unsup_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Qiz7TuhfJw_",
        "outputId": "5d4551d3-b1b1-491b-ac17-cc28ce770713"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "125325"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unsup_sentences[0:1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oy1zcR2lfNCU",
        "outputId": "3f9d2c25-21e8-487a-9d27-66e645f3fe59"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TaggedDocument(words=['once', 'upon', 'a', 'time', 'there', 'was', 'a', 'director', 'by', 'the', 'name', 'of', 'james', 'he', 'brought', 'us', 'wonderfully', 'thrilling', 'science', 'fiction', 'such', 'as', 'terminator', 'and', 'aliens', 'these', 'movies', 'were', 'the', 'stuff', 'blockbusters', 'were', 'made', 'of', 'and', 'he', 'looked', 'to', 'have', 'a', 'fantastic', 'future', 'ahead', 'of', 'him', 'as', 'the', 'dawn', 'of', 'computer', 'generated', 'special', 'effects', 'landed', 'upon', 'the', 'film', 'industry', 'terminator', '2', 'showed', 'gave', 'us', 'glimpses', 'of', 'what', 'was', 'possible', 'in', 'this', 'new', 'era', 'and', 'then', 'it', 'happened', '1997', 'countless', 'awards', 'obscene', 'amounts', 'of', 'money', 'outlandish', 'barrage', 'of', 'advertising', 'maximum', 'profit', 'margin', 'titanic', 'was', 'here', 'i', 'have', 'never', 'ever', 'been', 'one', 'to', 'jump', 'on', 'the', 'bandwagon', 'and', 'be', 'overly', 'critical', 'for', 'the', 'sake', 'of', 'it', 'in', 'fact', 'i', 'have', 'often', 'taken', 'the', 'opposite', 'stance', 'from', 'the', 'majority', 'just', 'to', 'get', 'an', 'argument', 'going', 'titanic', 'however', 'was', 'a', 'film', 'i', 'only', 'took', 'one', 'single', 'positive', 'out', 'of', 'that', 'of', 'kate', 'winslett', 'being', 'absolutely', 'gorgeous', 'throughout', 'quickly', 'the', 'dialogue', 'was', 'like', 'something', 'out', 'of', 'beverly', 'hills', '90210', 'the', 'acting', 'was', 'more', 'wooden', 'than', 'my', 'nephe', 'tree', 'house', 'images', 'meant', 'to', 'terrify', 'were', 'actually', 'comical', 'man', 'falling', 'from', 'ship', 'and', 'hitting', 'propeller', 'historically', 'false', 'do', 'even', 'get', 'me', 'started', 'because', 'ther', 'too', 'much', 'it', 'had', 'dire', 'theme', 'music', 'up', 'there', 'with', 'the', 'bodyguard', 'for', 'cheese', 'and', 'the', 'pointless', 'love', 'story', 'was', 'so', 'tedious', 'self', 'absorbing', 'and', 'pathetic', 'that', 'it', 'disrespected', 'the', 'plight', 'of', 'everyone', 'else', 'involved', 'i', 'was', 'glad', 'when', 'he', 'died', 'and', 'disappointed', 'when', 'she', 'did', 'not', 'it', 'was', 'plainly', 'obvious', 'from', 'the', 'word', 'go', 'that', 'this', 'picture', 'was', 'designed', 'to', 'appeal', 'to', 'mtv', 'watching', 'bubblegum', 'chewing', 'boy', 'with', 'car', 'chasing', 'teenage', 'girls', 'decaprio', 'himself', 'resembled', 'something', 'less', 'heroic', 'than', 'the', 'weedy', 'member', 'of', 'a', 'boy', 'band', 'who', 'would', 'drag', 'their', 'sex', 'starved', 'boyfriends', 'out', 'for', 'a', 'three', 'and', 'a', 'half', 'hour', 'chick', 'flick', 'hoping', 'to', 'get', 'lucky', 'later', 'the', 'worst', 'aspect', 'was', 'that', 'it', 'did', 'not', 'stop', 'at', 'that', 'point', 'millions', 'of', 'dumbed', 'down', 'culture', 'vultures', 'went', 'to', 'see', 'this', 'expensive', 'waste', 'of', 'celluloid', 'because', 'it', 'cost', 'so', 'much', 'to', 'produce', 'it', 'must', 'be', 'great', 'and', 'steve', 'and', 'barbara', 'said', 'it', 'was', 'good', 'and', 'they', 'know', 'their', 'movies', 'the', 'crowning', 'glory', 'arrived', 'when', 'titanic', 'swept', 'the', 'boards', 'at', 'the', 'academy', 'awards', 'king', 'james', 'of', 'hollywood', 'had', 'a', 'serious', 'moment', 'of', 'silence', 'for', 'the', 'victims', 'of', 'the', 'fatal', 'evening', 'on', 'which', 'his', 'three', 'and', 'a', 'half', 'hour', 'farce', 'was', 'based', 'it', 'looked', 'to', 'me', 'as', 'if', 'he', 'was', 'praying', 'for', 'forgiveness', 'after', 'making', 'a', 'fortune', 'off', 'inaccurately', 'portraying', 'the', 'circumstances', 'that', 'lead', 'to', 'the', 'death', 'of', 'a', 'lot', 'of', 'people', 'however', 'if', 'people', 'are', 'stupid', 'and', 'sentimental', 'enough', 'to', 'buy', 'into', 'this', 'kind', 'of', 'rubbish', 'they', 'deserve', 'to', 'get', 'ripped', 'off', 'good', 'luck', 'to', 'hollywood', 'if', 'that', 'is', 'how', 'they', 'want', 'to', 'make', 'money', 'do', 'it', 'if', 'i', 'had', 'those', 'kind', 'of', 'chances', 'in', 'life', 'it', 'is', 'right', 'up', 'there', 'on', 'my', 'all', 'time', 'worst', 'movies', 'list', 'with', 'other', 'silly', 'historically', 'false', 'human', 'interest', 'tripe', 'like', 'the', 'patriot', 'and', 'pearl', 'harbor'], tags=['/tmp/aclImdb/train/neg/3142_1.txt'])]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class PermuteSentences"
      ],
      "metadata": {
        "id": "PZn_WEHrEMhz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PermuteSentences(object):\n",
        "  # constructor\n",
        "  def __init__(self, sentences):\n",
        "    self.sentences=sentences\n",
        "\n",
        "  def __iter__(self):\n",
        "    shuffled = list(self.sentences)\n",
        "    random.shuffle(shuffled)\n",
        "    for sentence in shuffled:\n",
        "      yield sentence"
      ],
      "metadata": {
        "id": "351HUXU7f64g"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inisiliasi class PermuteSentences\n",
        "Melakukan inisialiasi class permute sentences dan membuat model"
      ],
      "metadata": {
        "id": "rp7EUbiJES5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "permuter = PermuteSentences(unsup_sentences)\n",
        "model = Doc2Vec(permuter, dm=0, hs=1, vector_size=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5H2xbGCgbEd",
        "outputId": "8fff53bd-bc6c-4bde-be5a-00db9bbcb9c6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Both hierarchical softmax and negative sampling are activated. This is probably a mistake. You should set either 'hs=0' or 'negative=0' to disable one of them. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/reviews.d2v')"
      ],
      "metadata": {
        "id": "uZ8aCYGhh16y"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip -O /tmp/sentiment_labelled_sentences.zip\n",
        "!unzip /tmp/sentiment_labelled_sentences.zip -d /content/data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cah0aL13mgMn",
        "outputId": "15c6c28a-f3d3-4766-d464-d13b9058b664"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-10 16:32:13--  https://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘/tmp/sentiment_labelled_sentences.zip’\n",
            "\n",
            "\r          /tmp/sent     [<=>                 ]       0  --.-KB/s               \r/tmp/sentiment_labe     [ <=>                ]  82.21K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-05-10 16:32:13 (3.23 MB/s) - ‘/tmp/sentiment_labelled_sentences.zip’ saved [84188]\n",
            "\n",
            "Archive:  /tmp/sentiment_labelled_sentences.zip\n",
            "   creating: /content/data/sentiment labelled sentences/\n",
            "  inflating: /content/data/sentiment labelled sentences/.DS_Store  \n",
            "   creating: /content/data/__MACOSX/\n",
            "   creating: /content/data/__MACOSX/sentiment labelled sentences/\n",
            "  inflating: /content/data/__MACOSX/sentiment labelled sentences/._.DS_Store  \n",
            "  inflating: /content/data/sentiment labelled sentences/amazon_cells_labelled.txt  \n",
            "  inflating: /content/data/sentiment labelled sentences/imdb_labelled.txt  \n",
            "  inflating: /content/data/__MACOSX/sentiment labelled sentences/._imdb_labelled.txt  \n",
            "  inflating: /content/data/sentiment labelled sentences/readme.txt  \n",
            "  inflating: /content/data/__MACOSX/sentiment labelled sentences/._readme.txt  \n",
            "  inflating: /content/data/sentiment labelled sentences/yelp_labelled.txt  \n",
            "  inflating: /content/data/__MACOSX/._sentiment labelled sentences  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing model"
      ],
      "metadata": {
        "id": "X1EztPxhEj9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test model\n",
        "sentences = []\n",
        "sentvecs = []\n",
        "sentiments = []\n",
        "for filename in ['yelp', 'amazon_cells', 'imdb']:\n",
        "  file_path = os.path.join('/content/data/sentiment labelled sentences', '%s_labelled.txt' % filename)\n",
        "  with open(file_path, encoding='utf-8') as f:\n",
        "    for i, line in enumerate(f):\n",
        "      line_split = line.strip().split('\\t')\n",
        "      sentences.append(line_split[0])\n",
        "      words = word_extract(line_split[0])\n",
        "      sentvecs.append(model.infer_vector(words, epochs=10))\n",
        "      sentiments.append(int(line_split[1]))\n",
        "\n",
        "# shuffling\n",
        "combined = list(zip(sentences, sentvecs, sentiments))\n",
        "random.shuffle(combined)\n",
        "sentences, sentvecs, sentiments = zip(*combined)"
      ],
      "metadata": {
        "id": "kPU5vvO2iJi0"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = KNeighborsClassifier(n_neighbors=9)\n",
        "clfrf = RandomForestClassifier()"
      ],
      "metadata": {
        "id": "pgOAUyj-m3jD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Menghitung / Kalkulasi nilai rata-rata dan standard devisiasi"
      ],
      "metadata": {
        "id": "XjWsus_GFKkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "score = cross_val_score(clf, sentvecs, sentiments, cv=5)\n",
        "# menghitung mean(rata-rata)\n",
        "# menghitung standard devisiasi\n",
        "np.mean(score), np.std(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCtzHPJynIQb",
        "outputId": "b24c3a17-b9dd-44d3-b604-cc642d665681"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.782, 0.017461067804945055)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score = cross_val_score(clfrf, sentvecs, sentiments, cv=5)\n",
        "# menghitung mean(rata-rata)\n",
        "# menghitung standard devisiasi\n",
        "np.mean(score), np.std(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RfgCDowntVY",
        "outputId": "8c565e80-8231-462a-e84d-0283732cb84f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7946666666666667, 0.01790096210946341)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = make_pipeline(CountVectorizer(), TfidfTransformer(), RandomForestClassifier())\n",
        "score = cross_val_score(clfrf, sentvecs, sentiments, cv=5)\n",
        "np.mean(score), np.std(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mkeOEZ-n4Sg",
        "outputId": "8343f3ec-3352-496a-cde6-8641398bd9b3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7929999999999999, 0.013391539617733778)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}